{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import traceback\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set a fixed seed for reproducibility\n",
    "# random.seed(42)\n",
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-3.5-turbo\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prompt template to summarize the requirements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_json_format = \"\"\"\n",
    "{\n",
    "    \"summarized_requirements\": [\n",
    "        {\n",
    "            \"id\": \"Serial Number\",\n",
    "            \"Requirement Name\": \"Give a name to the requirement in the format High_Level_Functionality_Low_Level_Functionality\",\n",
    "            \"Description\": \"Provide a description of the requirement\",\n",
    "            \"Priority\": \"Assign a priority to the requirement based on the criticality\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"Serial Number\",\n",
    "            \"Requirement Name\": \"Give a name to the requirement in the format High_Level_Functionality_Low_Level_Functionality\",\n",
    "            \"Description\": \"Provide a description of the requirement\",\n",
    "            \"Priority\": \"Assign a priority to the requirement based on the criticality\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = \"\"\"\n",
    "As a product owner, your responsibility is to distill the provided requirements into a concise and comprehensive summary. \\\n",
    "Your task is to generate appropriate requirements based on the {requirements}, ensuring that all aspects mentioned in the \\\n",
    "requirements are adequately covered.\n",
    "\n",
    "Generate as many requirements as necessary to encapsulate the specified functionality, ensuring that each aspect is \\\n",
    "addressed without duplication. Avoid adding any additional information beyond what is provided in the original requirements.\n",
    "\n",
    "Finally, format your response according to the provided summary_json_format given below and use it as a guide.\n",
    "{summary_json_format}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"requirements\",\"summary_response_json\"],\n",
    "    template=summarize_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = summarize_prompt,\n",
    "    output_key = \"summarized_requirements\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_json_format = \"\"\"\n",
    "{\n",
    "    \"test_cases\": [\n",
    "        {\n",
    "            \"test_case_name\": \"TC001_Functionality_LowLevelFunctionality_TestCaseType_Positive\",\n",
    "            \"pre_condition\": \"Condition to start the execution of the test case\",\n",
    "            \"steps_to_execute\": \"Steps to execute the test case with each step numbered\",\n",
    "            \"expected_result\": \"Desired result expected when the test case is executed\"\n",
    "        },\n",
    "        {\n",
    "            \"test_case_name\": \"TC002_High_Level_Functionality_LowLevelFunctionality_TestCaseType_Negative\",\n",
    "            \"pre_condition\": \"Condition to start the execution of the test case\",\n",
    "            \"steps_to_execute\": \"Steps to execute the test case with each step numbered\",\n",
    "            \"expected_result\": \"Desired result expected when the test case is executed\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_generation_template = \"\"\"\n",
    "As an experienced software tester, your task is to create comprehensive test cases based on the provided \\\n",
    "user requirements outlined below. Utilize industry-standard testing techniques such as Boundary Value Analysis, \\\n",
    "Equivalence Partitioning, Decision Table Testing, and Use Case Testing to ensure thorough coverage of both positive \\\n",
    "and negative scenarios.\n",
    "\n",
    "For each user requirement listed in {summarized_requirements}, systematically generate multiple test cases that validate the \\\n",
    "functionality under various conditions. Ensure to cover edge cases, error handling, and boundary scenarios to maximize \\\n",
    "test coverage.\n",
    "\n",
    "Finally, format your response according to the tc_json_format template provided below and use it as a guide. Do not add \\\n",
    "any additional character on your own.\n",
    "{tc_json_format}\n",
    "\n",
    "Please proceed with your test case generation, keeping the above instructions in mind.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"tc_json_format\", \"summarized_requirements\"],\n",
    "    template=tc_generation_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = tc_generation_prompt,\n",
    "    output_key = \"test_cases\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Traceability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_json_format = \"\"\"\n",
    "{\n",
    "    \"traceability_matrix\": [\n",
    "        {\n",
    "            \"User Requirement Id\": \"Requirement id from summarized_requirements e.g. 01\",\n",
    "            \"User Requirement Name\": \"Name of the requirement from summarized_requirements\",\n",
    "            \"Test Case Name\": \"Test case name from test_cases\"\n",
    "        },\n",
    "        {\n",
    "            \"User Requirement Id\": \"Requirement id from summarized_requirements e.g. 02\",\n",
    "            \"User Requirement Name\": \"Name of the requirement from summarized_requirements\",\n",
    "            \"Test Case Name\": \"Test case name from test_cases\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_generation_template = \"\"\"\"\n",
    "Your role as a software tester entails the creation of a traceability matrix to establish clear links between user \\\n",
    "requirements and the corresponding test cases. This matrix serves as a crucial document for ensuring comprehensive \\\n",
    "test coverage and validating that all requirements are adequately addressed.\n",
    "\n",
    "To construct the traceability matrix, meticulously match each user requirement listed in {summarized_requirements} \\\n",
    "to the relevant test case names listed in {test_cases}. If a single requirement is covered by multiple test cases, \\\n",
    "generate a distinct entry for each test case to maintain granularity.\n",
    "\n",
    "Lastly, organize your response according to the specified tm_json_format template provided below. Do not add any additional character on your own.\n",
    "Ensure that the resulting matrix accurately reflects the relationships between requirements and test cases. \n",
    "\n",
    "{tm_json_format}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"summarized_requirements\", \"test_cases\"],\n",
    "    template=tm_generation_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = tm_generation_prompt,\n",
    "    output_key = \"traceability_matrix\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_chain = SequentialChain(\n",
    "    chains= [summary_chain, tc_chain, tm_chain],\n",
    "    input_variables= [\"requirements\",\"summary_json_format\", \"tc_json_format\", \"tm_json_format\"],\n",
    "    output_variables= [\"summarized_requirements\", \"test_cases\", \"traceability_matrix\"],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"C:\\Documents\\AI\\GenAI\\E2E Test Case generator\\input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file_path, 'r') as file:\n",
    "    requirements = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = output_chain(\n",
    "        {\n",
    "            \"requirements\": requirements,\n",
    "            \"summary_json_format\": json.dumps(summary_json_format),\n",
    "            \"tc_json_format\": json.dumps(tc_json_format),\n",
    "            \"tm_json_format\": json.dumps(tm_json_format),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "print(f\"Propt Tokens: {cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "print(f\"Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_requirements = response.get(\"summarized_requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalized_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_json_data=json.loads(finalized_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = req_json_data['summarized_requirements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df = pd.DataFrame(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_df.to_csv(\"requirements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_test_cases = response.get(\"test_cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_test_cases=finalized_test_cases.strip()\n",
    "finalized_test_cases=finalized_test_cases.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalized_test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_json_data=json.loads(finalized_test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = tc_json_data['test_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_df = pd.DataFrame(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_df.to_csv(\"test_cases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_traceability_matrix = response.get(\"traceability_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_json_data=json.loads(finalized_traceability_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = tm_json_data['traceability_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_df = pd.DataFrame(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_df.to_csv(\"traceability_matrix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
